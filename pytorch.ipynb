{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch import  optim\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    def forward(self,x):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "# TODO:define model\n",
    "   def __init__(self,num_classes=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3),   #26 26 16\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3),    #24 24 32\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  #12 12 32\n",
    "            nn.Conv2d(32, 16, kernel_size=5),  # 8 8 16\n",
    "            nn.ReLU(inplace=True)\n",
    "            #\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),  # 4 4 128\n",
    "\n",
    "\n",
    "        )  #12 12 32\n",
    "            # nn.Conv2d(32, 128, kernel_size=3),  # 10 10 128\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Conv2d(128, 256, kernel_size=3), #8 8 256\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2))#  4 4 256\n",
    "\n",
    "        self.full_Connected=nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(8*8*16, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes)    )\n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=self.full_Connected(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "model = SimpleNet()\n",
    "\n",
    "# TODO:define loss function and optimiter\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train and evaluate\n",
    "losses =[]\n",
    "acces = []\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs=Variable(inputs)\n",
    "        labels=Variable(labels)\n",
    "        optimizer.zero_grad()       #\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)  # 计算loss\n",
    "        loss.backward()     # loss 求导\n",
    "        optimizer.step()    # 更新参数\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, pred = outputs.max(1)\n",
    "        num_correct = (pred == labels).sum().item()\n",
    "        acc = num_correct / inputs.shape[0]\n",
    "\n",
    "        train_acc += acc\n",
    "\n",
    "    losses.append(train_loss / len(train_loader))\n",
    "    acces.append(train_acc / len(train_loader))\n",
    "\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)  # 计算loss\n",
    "\n",
    "\n",
    "        eval_loss += loss.item()\n",
    "        _, pred = outputs.max(1)\n",
    "        num_correct = (pred == labels).sum().item()\n",
    "        acc = num_correct / inputs.shape[0]\n",
    "\n",
    "        eval_acc += acc\n",
    "    eval_losses.append(eval_loss / len(test_loader))\n",
    "    eval_acces.append(eval_acc / len(test_loader))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    # evaluate\n",
    "    # TODO:calculate the accuracy using traning and testing dataset\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d3a4b27378dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m print('Epoch {} Train Loss {} Train  Accuracy {} Teat Loss {} Test Accuracy {}'.format(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     eval_acc / len(test_loader)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "    print('Epoch {} Train Loss {} Train  Accuracy {} Teat Loss {} Test Accuracy {}'.format(\n",
    "        epoch + 1, train_loss / len(train_loader), train_acc / len(train_loader), eval_loss / len(test_loader),\n",
    "        eval_acc / len(test_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
